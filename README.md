# Метрические алгоритмы классификации

Метрический алгоритм - алгоритм классификации основанный на оценке близости объектов, используя функцию расстояния.
При этом функция расстояния не обязательно должна быть метрикой, то есть возможно нарушение неравенства треугольника

При этом метрические алгоритмы основываются на гипотезе компактности, которяа говорит что схожие объекты чаще лежат в одном классе, чем в разных, мера сходства здесь - введенное расстояние.

# KNN

- **KNN** - метрический алгоритм классификации, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки. 

Данный алгоритм, как и пара следующих рассмотренных методов, основываются на _**гипотезе компактности:**_ если мера сходства объектов введена достаточно удачно, то схожие объекты гораздо чаще лежат в одном классе, чем в разных. В этом случае граница между классами имеет достаточно простую форму, а классы образуют компактно локализованные области в пространстве объектов. 

Пусть задана обучающая выборка пар «объект-ответ».![](http://latex.codecogs.com/gif.latex?X%5Em%3D%5C%7B%28x_1%2Cy_1%29%2C%5Cdots%2C%28x_m%2Cy_m%29%5C%7D)

Пусть на множестве объектов задана функция расстояния ![](http://latex.codecogs.com/gif.latex?%5Crho%28x%2Cx%27%29). Эта функция должна быть достаточно точной _"мерой"_ сходства объектов.
Для точки **_u_** выборки отсортируем остальные её объекты по возрастанию значения расстояния до **_u_**.
Для успешного обучения выборка должна быть пересортирована для каждого новой точки **_u_**. 

В общем виде алгоритм **_kNN_** выглядит так :
![](http://latex.codecogs.com/gif.latex?a%28u%29%3D%5Cmathrm%7Barg%7D%5Cmax_%7By%5CinY%7D%5Csum_%7Bi%3D1%7D%5Em%5Cbigl%5Bx_%7Bi%3Bu%7D%3Dy%5Cbigr%5Dw%28i%2Cu%29),
где ![](http://latex.codecogs.com/gif.latex?w%28i%2Cu%29) — мера _«важности»_ (вес) объекта ![](http://latex.codecogs.com/gif.latex?x_u%5E%7Bi%7D)

Алгоритм зависит от параметра _k_, оптимальное значение которого определяется по критерию скользящего контроля,в нашем случае используется метод исключения объектов по одному (leave-one-out cross-validation).

Ниже график для LOO kNN и карта классификации

![](https://github.com/AJereli/SMPR/blob/master/imgs/knn.png)

## Парзеновское окно
Парзеновское окно - метрический алгоритм, частный случай метрических алгоритмов

h - параметр характеризующий ширину окна.

Оптимальное значение ℎ находим скользящим контролем, в частности leave-one-out

Алгоритм применялся для задачи классификации Ирисов Фишера по признакам Petal.Length
и Petal.Width

### Количество ошибок в LOO при соответствующих ядрах
- Епанечникова = 6 
- Квартическое = 6 
- Треуольное = 6 
- Гауссовское = 6
- Прямоугольное = 6

![](https://github.com/AJereli/SMPR/blob/master/grafic.png)

![](figures/img.png)

## kwNN
kwNN - метод взвешенных ближайших соседей, в отличии от kNN, оценивает степень важность каждого объекта обучающей выборки,
используя параметр w - вес объекта, где w = q^i. q - некоторое число ( 0 < q < 1). При q = 1, kwNN выраждается в kNN

Значение q находим методом LOO

![](https://github.com/AJereli/SMPR/blob/master/kwnn_g.png)

![](figures/img.png)

# Potential functions

- **Метод потенциальных функций** - метрический алгоритм классификации, основанный на идее электростатического взаимодействия элементарных частиц. В этой идее основным оперируемым понятием является _потенциал_, мера воздействия электростатического поля элементарной заряженной частицы (в некоторой точке пространства, конечно) , которая равна отношению ***заряда*** частицы **Q** к расстоянию до частицы ***(r)*** : ![](https://latex.codecogs.com/gif.latex?%5Cphi%20%28r%29%20%5Csim%20%5Cfrac%7BQ%7D%7Br%7D)

Данный метод при классификации объекта проверяет его на близость к объектам их выборки. Предполагается, что оные _"заряжены"_ классом, к которому они принадлежат, и мера важности зависит от заряда и расстояния до классифицируемого объекта.  

В общем виде алгоритм **_kNN_** выглядит так :
![](http://latex.codecogs.com/gif.latex?a%28u%29%3D%5Cmathrm%7Barg%7D%5Cmax_%7By%5CinY%7D%5Csum_%7Bi%3D1%7D%5Em%5Cbigl%5Bx_%7Bi%3Bu%7D%3Dy%5Cbigr%5Dw%28i%2Cu%29),
где ![](http://latex.codecogs.com/gif.latex?w%28i%2Cu%29) — мера _«важности»_ (вес) объекта ![](http://latex.codecogs.com/gif.latex?x_u%5E%7Bi%7D)

Метод же потенциальных функций заключается в использовании весовой функции 
![](https://latex.codecogs.com/gif.latex?w%28i%2Cu%29%20%3D%20%5Cgamma%20%28x%5Ei_u%29K%5Cleft%20%28%20%5Cfrac%7B%5Crho%28u%2Cx_u%28i%29%29%20%7D%7Bh%28x_u%28i%29%29%7D%20%5Cright%20%29), где    
K - заданная, убывающая с ростом аргумента, функция. Аналог ядра из метода Парзеновского окна;  
_p_ - расстояние от u до _i_-го ближайшего объекта;  
h - ширина потенциала, опять же, аналог с PW;  
![](https://latex.codecogs.com/gif.latex?%5Cgamma) - "заряд", степень важность объекта выборки при классификации.

Использована библиотека **plotrix** и её методы для отрисовки окружностей с заданной плотностью

### Summary

![](https://github.com/AJereli/SMPR/blob/master/imgs/pt.png)

----

# Линейные алгоритмы классификации
Линейный классификатор — алгоритм классификации, основанный на построении линейной разделяющей поверхности. В случае двух классов разделяющей поверхностью является гиперплоскость, которая делит пространство признаков на два полупространства.

Настройка линейного классификатора происходит методом минимизации эмпирического риска


## Персептрон Розенблатта
Персептрон Розенблатта — линейный классификатор,обучаемый с помощью стохастического градиента с правилом Хэбба и
кусочно-линейной функции потерь:

Персептрон обучают по правилу Хебба. Предъявляем на вход один объект. Если выходной сигнал персептрона совпадает с правильным ответом, то никаких действий предпринимать не надо. В случае ошибки необходимо обучить персептрон правильно решать данный пример.

Построим персептрон используя в качестве обучающей выборки ирисы Фишера, для простоты будем использовать только классы virginica и setosa помеченные как -1 и 1 соответственно

Покажем как персепртрон находит разделяющую гиперплоскость на разных итерациях

![](https://github.com/AJereli/SMPR/blob/master/steps.png)

Оптимально разделяющая гиперплоскость 

![](https://github.com/AJereli/SMPR/blob/master/full.png)

## ADALINE (Адаптивный линейный элемент)
Схема обучения ADALINE соответствует схеме обучения линейного классификатора методом стохастического градиент

В качестве функции потерь используется квадратичная функция потерь:

![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%28M%29%20%3D%20%28M%20-%201%29%5E2%20%3D%20%28%5Clangle%20w%2Cx_i%20%5Crangle%20y_i%20-%201%29%5E2). - квадратичная функция потерь.
Производная берётся по _w_ и равна ![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%27%28M%29%20%3D%202%28%5Clangle%20w%2Cx_i%20%5Crangle%20-%20y_i%29x_i).  
Получили правило обновления весов на каждой итерации метода 
![](http://latex.codecogs.com/svg.latex?w%20%3D%20w%20-%20%5Ceta%28%5Clangle%20w%2Cx_i%20%5Crangle%20-%20y_i%29x_i).

Это правило предложено Видроу и Хоффом и называется дельта-правилом

![](https://github.com/AJereli/SMPR/blob/master/imgs/adaline.png)

## Логистическая регрессия
Это линейный алгоритм классификации, также являющийся оптимальным байесовским. Как и предыдущие, использует **SG** и также основан на довольно сильных вероятностных предположениях. Функция потерь - логистическая:
![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%28M%29%20%3D%20%5Clog_2%281%20&plus;%20e%5E%7B-M%7D%29).

Правило обновления весов тоже другое, *логистическое*:
![](http://latex.codecogs.com/svg.latex?w%20%3A%3D%20w&plus;%5Ceta%20y_ix_i%5Csigma%28-%5Clangle%20w%2Cx_i%20%5Crangle%20y_i%29)
, а ![](http://latex.codecogs.com/svg.latex?%5Csigma%28-M_i%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E%7BM_i%7D%7D) - сигмоидная функция.

![](https://github.com/AJereli/SMPR/blob/master/imgs/logreg.png)

## SVM (Support vector machine)
Метод опорных векторов

В настоящее время метод опорных векторов (SVM) считается одним из лучших методов классификации! Метод SVM обладает несколькими замечательными свойствами:

Обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках в сотни тысяч объектов;
Решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой доли обучающих объектов. Они и называются опорными векторами, остальные объекты фактически не задействуются; СМПР
С помощью функции ядра метод обобщается на случай нелинейных разделяющих поверхностей. Вопрос о выборе ядра, оптимального для данной прикладной задачи, до сих пор остается открытой теоретической проблемой.

Чтобы протестировать метод будем использовать библиотеку kernlab и датасет spam (электронные письма)

Результаты классификации тестовых данных

| mailtype | nonspam | spam  |
|-----------|------------------------------------------|-------------------------------------------------------------------------|
| nonspam  | 1360 | 106|
| spam  | 59 | 775 |


mailtype  nonspam spam
  nonspam    1360  106
  spam         59  775

Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 

Gaussian Radial Basis kernel function.

 Hyperparameter : sigma =  0.0274333299749945 

Number of Support Vectors : 841 

Objective Function Value : -467.9232 

Training error : 0.045217 


# Байесовские алгоритмы

## Наивный нормальный байесовский классификатор

Все объекты описываются _n_ числовыми признаками, ![](http://latex.codecogs.com/svg.latex?x%20%3D%20%28%5Cxi_1%2C...%2C%5Cxi_n%29). Сама наивность классификатора состоит в предположении, что все признаки ![](http://latex.codecogs.com/svg.latex?f_1%2C...%2Cf_n) - независимые случайные величины, и тогда функции правдоподобия классов (совместные плотности) : ![](http://latex.codecogs.com/svg.latex?p_y%28x%29%20%3D%20p_%7By1%7D%28%5Cxi_1%29...p_%7Byn%7D%28%5Cxi_n%29%2C%20y%20%5Cin%20Y), где ![](http://latex.codecogs.com/svg.latex?p_%7Byj%7D%28%5Cxi_j%29) - плотность распределений значений _j_-го признака для класса _y_.  

Оценка **n** одномерных плотностей проще, чем одной n-мерной. Подставляя оценки ![](http://latex.codecogs.com/svg.latex?p_%7Byj%7D%28%5Cxi_j%29) в _ОБРП_, получим байесовский классификатор:  

![](http://latex.codecogs.com/svg.latex?a%28x%29%20%3D%20%5Carg%5Cmax%5CBigr%28%5Cln%5Clambda_y%5Cwidehat%7BP%7D_y%20&plus;%20%5Csum_%7Bi%3D1%7D%5En%5Cln%5Cwidehat%7Bp%7D_%7Byj%7D%28%5Cxi_j%29%5CBigr%29)

В случае, если классы равнозначны ![](http://latex.codecogs.com/svg.latex?%5Clambda_y%5Cequiv%201), байесовское правило - ***принцип максимума апостериорной вероятности***. Если они ещё и равновероятны ![](http://latex.codecogs.com/svg.latex?P_y%20%5Cequiv%20%5Cfrac%7B1%7D%7B%7CY%7C%7D) (это и предыдущее условие выполняется для выборки ирисов Фишера), то объект относится к классу с наибольшим значением совместной плотности ![](http://latex.codecogs.com/svg.latex?p_y%28x%29) в точке.  

Всё, что требуется сделать - посчитать плотности на каждом признаке элемента, перемножить их, повторить для всех классов и найти максимальную. Плотность у нас нормальная (гауссовская):
![](http://latex.codecogs.com/svg.latex?p_%7Byj%7D%28%5Cxi%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csigma_%7Byj%7D%5Csqrt%7B2%5Cpi%7D%7D%5Cexp%5CBigr%28-%5Cfrac%7B%28%5Cxi-%5Cmu_%7Byj%7D%29%5E2%7D%7B2%5Csigma%5E2_%7Byj%7D%20%7D%5CBigr%29), где ![](http://latex.codecogs.com/svg.latex?%5Cmu_%7Byj%7D) - матожидание j-го признака класса y, ![](http://latex.codecogs.com/svg.latex?%5Csigma%5E2_%7Byj%7D) - дисперсия j-го признака класса y.

![](https://github.com/AJereli/SMPR/blob/master/imgs/bayes.png)

### Линейный дискриминант Фишера 
Допустим равеноство ковариацонных матриц классов ![equation](http://latex.codecogs.com/gif.latex?\sum). 

Оценим ![equation](http://latex.codecogs.com/gif.latex?\sum^{-}) по всем l объектам обучающей выборке. С учетом поправки на смещённость,

![equation](http://latex.codecogs.com/gif.latex?\sum^{-}&space;=&space;\frac{1}{l-|Y|}\sum_{i=1}^l&space;(x_i&space;-&space;\mu^{-}_{y_i})(x_i&space;-&space;\mu^{-}_{y_i})^T)

Тогда разделяющая поверхность линейна. Подстановочный алгоритм имеет вид:

![equation](http://latex.codecogs.com/gif.latex?a(x)=argmax_{y\epsilon&space;Y}\lambda_y&space;P_y&space;\rho_y&space;(x)&space;=&space;argmax_{y\epsilon&space;Y}&space;(\ln&space;(\lambda_y&space;P_y)&space;-&space;\frac{1}{2}&space;\mu_y^{T}&space;\sum^{-1}&space;\mu_y&space;&plus;&space;x^{T}&space;\sum^{-1}&space;\mu_y))

Полученный алгоритм называется линейным дискриминантом Фишера. ЛДФ показывается хорошие результаты если формы классов близки к нормальных и схожи. В этом случае линейное решающее правило близко к оптимальному байесовскому, но существенно более устойчиво, чем квадратичное, и часто обладает лучшей обобщающей способностью.

Вероятность ошибки линейного дискриминанта Фишера выражается через расстояние Махаланобиса между классами, в случае, когда классов два:
![equation](http://latex.codecogs.com/gif.latex?R(a)&space;=&space;\Phi&space;(-\frac{1}{2}||\mu_1&space;-&space;\mu_2||_{\sum}))
где ![equation](http://latex.codecogs.com/gif.latex?\Phi&space;(x)&space;=&space;N(x;0,1)) - функция стандартного нормального распределения.

Для теста использовался датасет spam 
На первом графике использовались признаки charDollar и metting

На втором графике признаки charDollar и capitalLong

![](https://github.com/AJereli/SMPR/blob/master/imgs/ldf.png)

![](https://github.com/AJereli/SMPR/blob/master/imgs/longLdf.png)
