# Метрические алгоритмы классификации

Метрический алгоритм - алгоритм классификации основанный на оценке близости объектов, используя функцию расстояния.
При этом функция расстояния не обязательно должна быть метрикой, то есть возможно нарушение неравенства треугольника

При этом метрические алгоритмы основываются на гипотезе компактности, которяа говорит что схожие объекты чаще лежат в одном классе, чем в разных, мера сходства здесь - введенное расстояние.

# KNN

- **KNN** - метрический алгоритм классификации, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки. 

Данный алгоритм, как и пара следующих рассмотренных методов, основываются на _**гипотезе компактности:**_ если мера сходства объектов введена достаточно удачно, то схожие объекты гораздо чаще лежат в одном классе, чем в разных. В этом случае граница между классами имеет достаточно простую форму, а классы образуют компактно локализованные области в пространстве объектов. 

Пусть задана обучающая выборка пар «объект-ответ».![](http://latex.codecogs.com/gif.latex?X%5Em%3D%5C%7B%28x_1%2Cy_1%29%2C%5Cdots%2C%28x_m%2Cy_m%29%5C%7D)

Пусть на множестве объектов задана функция расстояния ![](http://latex.codecogs.com/gif.latex?%5Crho%28x%2Cx%27%29). Эта функция должна быть достаточно точной _"мерой"_ сходства объектов.
Для точки **_u_** выборки отсортируем остальные её объекты по возрастанию значения расстояния до **_u_**.
Для успешного обучения выборка должна быть пересортирована для каждого новой точки **_u_**. 

В общем виде алгоритм **_kNN_** выглядит так :
![](http://latex.codecogs.com/gif.latex?a%28u%29%3D%5Cmathrm%7Barg%7D%5Cmax_%7By%5CinY%7D%5Csum_%7Bi%3D1%7D%5Em%5Cbigl%5Bx_%7Bi%3Bu%7D%3Dy%5Cbigr%5Dw%28i%2Cu%29),
где ![](http://latex.codecogs.com/gif.latex?w%28i%2Cu%29) — мера _«важности»_ (вес) объекта ![](http://latex.codecogs.com/gif.latex?x_u%5E%7Bi%7D)

Алгоритм зависит от параметра _k_, оптимальное значение которого определяется по критерию скользящего контроля,в нашем случае используется метод исключения объектов по одному (leave-one-out cross-validation).

Ниже график для LOO kNN и карта классификации

![](https://github.com/AJereli/SMPR/blob/master/imgs/knn.png)

## Парзеновское окно
Парзеновское окно - метрический алгоритм, частный случай метрических алгоритмов

h - параметр характеризующий ширину окна.

Оптимальное значение ℎ находим скользящим контролем, в частности leave-one-out

Алгоритм применялся для задачи классификации Ирисов Фишера по признакам Petal.Length
и Petal.Width

### Количество ошибок в LOO при соответствующих ядрах
- Епанечникова = 6 
- Квартическое = 6 
- Треуольное = 6 
- Гауссовское = 6
- Прямоугольное = 6

![](https://github.com/AJereli/SMPR/blob/master/grafic.png)

![](figures/img.png)

## kwNN
kwNN - метод взвешенных ближайших соседей, в отличии от kNN, оценивает степень важность каждого объекта обучающей выборки,
используя параметр w - вес объекта, где w = q^i. q - некоторое число ( 0 < q < 1). При q = 1, kwNN выраждается в kNN

Значение q находим методом LOO

![](https://github.com/AJereli/SMPR/blob/master/kwnn_g.png)

![](figures/img.png)

# Potential functions

- **Метод потенциальных функций** - метрический алгоритм классификации, основанный на идее электростатического взаимодействия элементарных частиц. В этой идее основным оперируемым понятием является _потенциал_, мера воздействия электростатического поля элементарной заряженной частицы (в некоторой точке пространства, конечно) , которая равна отношению ***заряда*** частицы **Q** к расстоянию до частицы ***(r)*** : ![](https://latex.codecogs.com/gif.latex?%5Cphi%20%28r%29%20%5Csim%20%5Cfrac%7BQ%7D%7Br%7D)

Данный метод при классификации объекта проверяет его на близость к объектам их выборки. Предполагается, что оные _"заряжены"_ классом, к которому они принадлежат, и мера важности зависит от заряда и расстояния до классифицируемого объекта.  

В общем виде алгоритм **_kNN_** выглядит так :
![](http://latex.codecogs.com/gif.latex?a%28u%29%3D%5Cmathrm%7Barg%7D%5Cmax_%7By%5CinY%7D%5Csum_%7Bi%3D1%7D%5Em%5Cbigl%5Bx_%7Bi%3Bu%7D%3Dy%5Cbigr%5Dw%28i%2Cu%29),
где ![](http://latex.codecogs.com/gif.latex?w%28i%2Cu%29) — мера _«важности»_ (вес) объекта ![](http://latex.codecogs.com/gif.latex?x_u%5E%7Bi%7D)

Метод же потенциальных функций заключается в использовании весовой функции 
![](https://latex.codecogs.com/gif.latex?w%28i%2Cu%29%20%3D%20%5Cgamma%20%28x%5Ei_u%29K%5Cleft%20%28%20%5Cfrac%7B%5Crho%28u%2Cx_u%28i%29%29%20%7D%7Bh%28x_u%28i%29%29%7D%20%5Cright%20%29), где    
K - заданная, убывающая с ростом аргумента, функция. Аналог ядра из метода Парзеновского окна;  
_p_ - расстояние от u до _i_-го ближайшего объекта;  
h - ширина потенциала, опять же, аналог с PW;  
![](https://latex.codecogs.com/gif.latex?%5Cgamma) - "заряд", степень важность объекта выборки при классификации.

Использована библиотека **plotrix** и её методы для отрисовки окружностей с заданной плотностью

### Summary

![](https://github.com/AJereli/SMPR/blob/master/imgs/pt.png)

----

# Линейные алгоритмы классификации
Линейный классификатор — алгоритм классификации, основанный на построении линейной разделяющей поверхности. В случае двух классов разделяющей поверхностью является гиперплоскость, которая делит пространство признаков на два полупространства.

Настройка линейного классификатора происходит методом минимизации эмпирического риска


## Персептрон Розенблатта
Персептрон Розенблатта — линейный классификатор,обучаемый с помощью стохастического градиента с правилом Хэбба и
кусочно-линейной функции потерь:

Персептрон обучают по правилу Хебба. Предъявляем на вход один объект. Если выходной сигнал персептрона совпадает с правильным ответом, то никаких действий предпринимать не надо. В случае ошибки необходимо обучить персептрон правильно решать данный пример.

Построим персептрон используя в качестве обучающей выборки ирисы Фишера, для простоты будем использовать только классы virginica и setosa помеченные как -1 и 1 соответственно

Покажем как персепртрон находит разделяющую гиперплоскость на разных итерациях

![](https://github.com/AJereli/SMPR/blob/master/steps.png)

Оптимально разделяющая гиперплоскость 

![](https://github.com/AJereli/SMPR/blob/master/full.png)

## ADALINE (Адаптивный линейный элемент)
Схема обучения ADALINE соответствует схеме обучения линейного классификатора методом стохастического градиент

В качестве функции потерь используется квадратичная функция потерь:

![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%28M%29%20%3D%20%28M%20-%201%29%5E2%20%3D%20%28%5Clangle%20w%2Cx_i%20%5Crangle%20y_i%20-%201%29%5E2). - квадратичная функция потерь.
Производная берётся по _w_ и равна ![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%27%28M%29%20%3D%202%28%5Clangle%20w%2Cx_i%20%5Crangle%20-%20y_i%29x_i).  
Получили правило обновления весов на каждой итерации метода 
![](http://latex.codecogs.com/svg.latex?w%20%3D%20w%20-%20%5Ceta%28%5Clangle%20w%2Cx_i%20%5Crangle%20-%20y_i%29x_i).

Это правило предложено Видроу и Хоффом и называется дельта-правилом

![](https://github.com/AJereli/SMPR/blob/master/imgs/adaline.png)
